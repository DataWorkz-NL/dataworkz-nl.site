---
layout: post
title:  "ODSC 2017 – trainingen, talks en workshops over Data Science topics"
date:   2015-11-17 16:16:01 -0600
categories: jekyll update
type: blog
icon: odsc_icon.png
---

Afgelopen week bezocht ik in Londen het ODSC, Open Data Science Conference. Een congres van drie dagen dat in het teken staat van de ontwikkelingen en trends op het gebied van Data Science. Ik heb twee van de drie dagen bezocht en er was een duidelijk verschil in bezoekersaantallen en dagindeling. 
Vooraf was aangekondigd dat het uitverkocht was met zo’n 1.500 bezoekers. De donderdag bleek uiteindelijk veruit de leukste en interessantste dag, omdat er uitgebreide trainingen werden gegeven van 4 uur lang. Laat dit nou net de dag zijn dat het bezoekersaantal vrij laag was, erg prettig om de trainingen te volgen in kleinere groepen. 
Vrijdag was het erg druk met in de ochtend drie keynote talks en na de lunch korte workshops en introducties van tools en trends.

Ik startte het congres met een training over R, gegeven door dr. Colin Gilespie van de University of Newcastle. Leuke training van 4 uur waarbij de focus met name lag op scaling en clustering (en het luisteren naar een Schot wat even een aantal minuten wennen was ;) ). Hij ging uitgebreid in op het belang van scaling wanneer je predicitive analysis of machine learning toepast. Daarbij kwamen nuttige functies en packages aan bod. Vooraf een klein beetje bang dat het niet veel  nieuws op zou leveren, maar het was een hele leerzame en nuttige mix van herhaling/opfrissing en voor mij nieuwe functies en trucs om scaling toe te passen. De bekende methodes zijn min-max scaling of standardisation (resulteren in mean 0 en variance 1). Niet bekend (voor mij in ieder geval) was Mahalanobis scaling. Hierbij wordt zowel de variantie als de covariantie gestandaardiseerd. Dit geeft nuttige toepassingen in clustering algoritmes. Naast Mahalanobis scaling is er het begrip Mahalanobis Distance als tegenhanger van de meer bekende Euclidean en Manhatten Distance. Bij de Mahalanobis Distance speelt de Convariantie Matrix een belangrijke rol. Het zorgt er voor dat data niet alleen compacter wordt (squashed), maar ook wordt geroteerd.  Leuke en duidelijke voorbeelden staan in slides die ik graag deel mocht je interesse hebben.

Een bijkomend voordeel van het lage bezoekersaantal op de donderdag werd duidelijk bij de lunch waar, zo leek het, wel was gerekend op fors meer mensen :).

Na de lunch stond een training Deep Learning in Google’s TensorFlow op het programma, gegeven door Kaz Sato, Machine Learning Expert bij Google. Vervelend was dat het meer dan een half uur duurde voordat iedereen bij de Google Cloud omgeving kon met de data, maar dat nam niet weg dat TensorFlow een bijzonder krachtige en handige tool lijkt te zijn om heel snel neural networks te maken. En dat met een paar regels Python code en zonder diep op de wiskundige achtergrond in te gaan. Het voorbeeld dat hier werd gegeven betreft het classificeren of een plek wel of niet in Manhatten ligt. Niet direct iets om deep learning op toe te passen, maar voor het idee een aardig voorbeeld. Hieronder een stukje code voor een neural network met vijf hidden layers met elke 20 units en twee output variabelen. Vervolgens wordt het model getraind waarbij iedere 30 stappen het resultaat tot dusver geplot wordt. Je ziet hier goed hoe weinig code daarvoor nodig is, uiteraard los van voorbereidend werk als het creëren van een trainingset en de features.

{% highlight python %}
dnnc = tf.contrib.learn.DNNClassifier(
  feature_columns=feature_columns,
  hidden_units=[20, 20, 20, 20],
  n_classes=2)
steps = 30
for i in range (1, 6):
  dnnc.fit(x=latlng_train, y=is_mt_train, steps=steps)
  plot_predicted_map()
  print 'Steps: ' + str(i * steps)
  print_accuracy()
{% endhighlight %}
 

Op de vrijdag werden in de ochtend drie keynote talks gegeven. Allereerst door Neil Lawrence, Director Machine Learning at Amazon Research. Hij praatte over professionalisering binnen Data Science, waarmee hij zich vooral richtte op het delen van kennis en expertise. Er worden overal methodes ontwikkeld en programma’s geschreven, maar hoe wordt geborgd dat alle best practices en leerpunten met elkaar gedeeld worden? Daarnaast stond hij stil bij het feit dat data preparatie ontzettend veel tijd kost en dat het goed zou zijn om ook dit te delen. 

Maak de geprepareerde data herbruikbaar.
Vervolgens een meer inhoudelijke presentatie over het inzetten van neural networks bij Machine Translations, bijvoorbeeld Google Translate, gegeven door Dr. John D. Kelleher van Dublin Institute of Technology. Er wordt hierbij gebruik gemaakt van een vector representatie van woorden om die vervolgens in een recurrent neural network te stoppen. In een regulier neural network zijn alle inputs onafhankelijk van elkaar, wat vaak wenselijk en voldoende is. Echter bij spraakherkenning of vertaling is het veel beter om te weten wat er eerder gezegd of geschreven is. Recurrent neural networks maken dit mogelijk. De zogenaamde hidden layer van het netwerk is bepaald door de uitkomst van een eerdere berekening. De output wordt daarmee afhankelijk van wat er eerder is berekend.

Tot slot nog een keynote presentatie van iemand van IBM, een van de grote sponsors van het evenement. Dit was in mijn ogen de minst interessante en met name een mooi promopraatje.

Na de lunch had ik, voor ik naar het vliegveld moest, nog tijd voor een workshop over Deep Learning in Keras. In deze korte workshop werd een tutorial gegeven over toepassing in beeldherkenning bij Badoo, een grote datingsite. Keras is een gebruikersvriendelijke neural network API die draait bovenop bijvoorbeeld TensorFlow. Het stelt je in staat om heel snel extra hidden layers toe te voegen in je netwerk om te ontdekken of de performance verbetert. Ik merkte tijdens de workshop dat het inderdaad eenvoudig is om snel een netwerk in elkaar te zetten en aan te passen, maar eerlijkheid gebied te zeggen dat het mij (nog) niet duidelijk is wat nou precies het voordeel is ten opzichte van TensorFlow.

De derde dag zat er voor mij niet meer in, omdat helaas het strand van Gran Canaria op mijn wachtte ;), maar de eerste twee dagen waren zeker de moeite waard en ik heb me voorgenomen om me meer te gaan verdiepen in TensorFlow.

Indien je interesse hebt over één of meer van deze onderwerpen, laat het me gerust weten. Ik heb slides over de R training en Jupyter Notebooks van de TensorFlow training.

#### **_[Olaf](mailto:olaf@dataworkz.nl)_**

